{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c526c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0bk_body\n",
      "Task WM, Condition 0bk_body: Concatenated array shape (339, 360, 78)\n",
      "0bk_body → shape: (339, 360, 78)\n",
      "Saved: /home/amirreza/neuromatch/hcp_task/concatenated_data/0bk_body.npy\n",
      "Processing: 0bk_faces\n",
      "Task WM, Condition 0bk_faces: Concatenated array shape (339, 360, 78)\n",
      "0bk_faces → shape: (339, 360, 78)\n",
      "Saved: /home/amirreza/neuromatch/hcp_task/concatenated_data/0bk_faces.npy\n",
      "Processing: 0bk_places\n",
      "Task WM, Condition 0bk_places: Concatenated array shape (339, 360, 78)\n",
      "0bk_places → shape: (339, 360, 78)\n",
      "Saved: /home/amirreza/neuromatch/hcp_task/concatenated_data/0bk_places.npy\n",
      "Processing: 0bk_tools\n",
      "Task WM, Condition 0bk_tools: Concatenated array shape (339, 360, 78)\n",
      "0bk_tools → shape: (339, 360, 78)\n",
      "Saved: /home/amirreza/neuromatch/hcp_task/concatenated_data/0bk_tools.npy\n",
      "Processing: 2bk_body\n",
      "Task WM, Condition 2bk_body: Concatenated array shape (339, 360, 78)\n",
      "2bk_body → shape: (339, 360, 78)\n",
      "Saved: /home/amirreza/neuromatch/hcp_task/concatenated_data/2bk_body.npy\n",
      "Processing: 2bk_faces\n",
      "Task WM, Condition 2bk_faces: Concatenated array shape (339, 360, 78)\n",
      "2bk_faces → shape: (339, 360, 78)\n",
      "Saved: /home/amirreza/neuromatch/hcp_task/concatenated_data/2bk_faces.npy\n",
      "Processing: 2bk_places\n",
      "Task WM, Condition 2bk_places: Concatenated array shape (339, 360, 78)\n",
      "2bk_places → shape: (339, 360, 78)\n",
      "Saved: /home/amirreza/neuromatch/hcp_task/concatenated_data/2bk_places.npy\n",
      "Processing: 2bk_tools\n",
      "Task WM, Condition 2bk_tools: Concatenated array shape (339, 360, 78)\n",
      "2bk_tools → shape: (339, 360, 78)\n",
      "Saved: /home/amirreza/neuromatch/hcp_task/concatenated_data/2bk_tools.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# information\n",
    "N_SUBJECTS = 339\n",
    "N_PARCELS = 360\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "N_RUNS = 2\n",
    "HCP_DIR = \"/home/amirreza/neuromatch/hcp_task\"  # Update to your data path\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'MOTOR': {'runs': [5, 6], 'cond': ['lf', 'rf', 'lh', 'rh', 't', 'cue']},\n",
    "    'WM': {'runs': [7, 8], 'cond': ['0bk_body', '0bk_faces', '0bk_places', '0bk_tools', '2bk_body', '2bk_faces', '2bk_places', '2bk_tools']},\n",
    "    'EMOTION': {'runs': [9, 10], 'cond': ['fear', 'neut']},\n",
    "    'GAMBLING': {'runs': [11, 12], 'cond': ['loss', 'win']},\n",
    "    'LANGUAGE': {'runs': [13, 14], 'cond': ['math', 'story']},\n",
    "    'RELATIONAL': {'runs': [15, 16], 'cond': ['match', 'relation']},\n",
    "    'SOCIAL': {'runs': [17, 18], 'cond': ['mental', 'rnd']}\n",
    "}\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_single_timeseries(subject, experiment, run, dir, remove_mean=True):\n",
    "   \"\"\"Load timeseries data for a single subject and single run.\n",
    "\n",
    "\n",
    "   Args:\n",
    "       subject (str): Subject ID to load\n",
    "       experiment (str): Name of experiment\n",
    "       run (int): 0-based run index (0 or 1 for the task)\n",
    "       dir (str): Path to HCP directory\n",
    "       remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "\n",
    "\n",
    "   Returns:\n",
    "       ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "   \"\"\"\n",
    "   bold_run = EXPERIMENTS[experiment]['runs'][run]\n",
    "   # Corrected path: Removed the extra \"hcp_task\"\n",
    "   bold_path = os.path.join(dir, \"subjects\", str(subject), \"timeseries\")\n",
    "   bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "   ts = np.load(os.path.join(bold_path, bold_file))\n",
    "   if remove_mean:\n",
    "       ts -= ts.mean(axis=1, keepdims=True)\n",
    "   return ts\n",
    "\n",
    "def load_evs(subject, experiment, run, dir):\n",
    "   \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "\n",
    "   Args:\n",
    "       subject (str): Subject ID to load\n",
    "       experiment (str): Name of experiment\n",
    "       run (int): 0-based run index (0 or 1 for the task)\n",
    "       dir (str): Path to HCP directory\n",
    "\n",
    "\n",
    "   Returns:\n",
    "       evs (list of lists): A list of frames associated with each condition, clipped to valid timepoints\n",
    "   \"\"\"\n",
    "   frames_list = []\n",
    "   task_key = 'tfMRI_' + experiment + '_' + ['RL', 'LR'][run]\n",
    "   # Load time series to get number of timepoints\n",
    "   try:\n",
    "       ts = load_single_timeseries(subject, experiment, run, dir, remove_mean=False)\n",
    "       n_timepoints = ts.shape[1]\n",
    "   except Exception as e:\n",
    "       print(f\"Error loading time series for {subject}/{experiment}/bold{EXPERIMENTS[experiment]['runs'][run]} to clip frames: {e}\")\n",
    "       return None\n",
    "\n",
    "\n",
    "   for cond in EXPERIMENTS[experiment]['cond']:\n",
    "       # Corrected path: Removed the extra \"hcp_task\"\n",
    "       ev_file = os.path.join(dir, \"subjects\", str(subject), \"EVs\", task_key, f\"{cond}.txt\")\n",
    "       try:\n",
    "           ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "           ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "           # Determine when trial starts, rounded down\n",
    "           start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "           # Use trial duration to determine how many frames to include for trial\n",
    "           duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "           # Take the range of frames that correspond to this specific trial\n",
    "           frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "           # Clip frames to valid timepoints\n",
    "           clipped_frames = []\n",
    "           for frame in frames:\n",
    "               valid_frames = frame[frame < n_timepoints]\n",
    "               if len(valid_frames) > 0:\n",
    "                   clipped_frames.append(valid_frames)\n",
    "           frames_list.append(clipped_frames if clipped_frames else [])\n",
    "       except Exception as e:\n",
    "           print(f\"Error loading EV file {ev_file}: {e}\")\n",
    "           frames_list.append([])\n",
    "\n",
    "\n",
    "   return frames_list\n",
    "\n",
    "def concatenate_task_data(task, condition, dir):\n",
    "   \"\"\"\n",
    "   Concatenate condition-specific frames from bold*.npy files for all subjects for a specific task and condition.\n",
    "   Extract frames using load_evs, concatenate across runs, truncate to minimum frame count across subjects,\n",
    "   and stack into a design matrix.\n",
    "\n",
    "   Args:\n",
    "       task (str): Task name (e.g., 'SOCIAL', 'WM')\n",
    "       condition (str): Condition name (e.g., 'mental', '0bk_body')\n",
    "       dir (str): Path to HCP directory\n",
    "\n",
    "   Returns:\n",
    "       task_cond_array (array): Array of shape (n_subjects, n_parcels, n_condition_timepoints)\n",
    "                               or None if no data is concatenated\n",
    "   \"\"\"\n",
    "   # Validate inputs\n",
    "   if task not in EXPERIMENTS:\n",
    "       print(f\"Invalid task: {task}. Must be one of {list(EXPERIMENTS.keys())}.\")\n",
    "       return None\n",
    "   if condition not in EXPERIMENTS[task]['cond']:\n",
    "       print(f\"Invalid condition: {condition} for task {task}. Must be one of {EXPERIMENTS[task]['cond']}.\")\n",
    "       return None\n",
    "\n",
    "   # Load subjects\n",
    "   subjects = [str(i) for i in range(339)]\n",
    "\n",
    "   task_data = []\n",
    "   frame_counts = []\n",
    "\n",
    "   # Iterate through subjects\n",
    "   for subject in subjects:\n",
    "       subject_folder = Path(dir) / \"subjects\" / str(subject)\n",
    "       if not subject_folder.is_dir():\n",
    "           print(f\"Subject directory {subject_folder} does not exist. Skipping.\")\n",
    "           continue\n",
    "\n",
    "       # Load time series and EV frames for both runs\n",
    "       subject_task_data = []\n",
    "       subject_frames = []\n",
    "       for run_idx in range(2):  # Two runs per task\n",
    "           try:\n",
    "               # Load time series\n",
    "               ts = load_single_timeseries(subject, task, run_idx, dir, remove_mean=False)\n",
    "               subject_task_data.append(ts)\n",
    "               # Load EV frames\n",
    "               frames_list = load_evs(subject, task, run_idx, dir)\n",
    "               if frames_list is None:\n",
    "                   raise Exception(\"EV frames are None\")\n",
    "               subject_frames.append(frames_list)\n",
    "           except Exception as e:\n",
    "               print(f\"Error loading data or EVs for {subject}/{task}/bold{EXPERIMENTS[task]['runs'][run_idx]}: {e}\")\n",
    "               subject_task_data.append(None)\n",
    "               subject_frames.append(None)\n",
    "\n",
    "       # Check if any run failed to load\n",
    "       if any(x is None for x in subject_task_data) or any(x is None for x in subject_frames):\n",
    "           print(f\"Skipping subject {subject} for task {task} due to missing data or EV frames.\")\n",
    "           continue\n",
    "\n",
    "       # Get index of condition in EXPERIMENTS[task]['cond']\n",
    "       cond_idx = EXPERIMENTS[task]['cond'].index(condition)\n",
    "\n",
    "       try:\n",
    "           # Concatenate frames for this condition across runs\n",
    "           cond_frames = []\n",
    "           for run_idx, run_frames in enumerate(subject_frames):\n",
    "               # Flatten the frame list for this condition and adjust for run concatenation\n",
    "               run_cond_frames = np.concatenate(run_frames[cond_idx]).astype(int) if run_frames[cond_idx] else np.array([])\n",
    "               if run_idx == 1 and subject_task_data[0] is not None and len(run_cond_frames) > 0:\n",
    "                   # Offset second run frames by the number of timepoints in first run\n",
    "                   run_cond_frames += subject_task_data[0].shape[1]\n",
    "               cond_frames.append(run_cond_frames)\n",
    "\n",
    "           # Combine frames from both runs\n",
    "           all_cond_frames = np.concatenate(cond_frames) if cond_frames and any(len(f) > 0 for f in cond_frames) else np.array([])\n",
    "\n",
    "           # Skip if no valid frames\n",
    "           if len(all_cond_frames) == 0:\n",
    "               print(f\"No valid frames for {subject}/{task}/{condition}. Skipping.\")\n",
    "               continue\n",
    "\n",
    "           # Concatenate time series for both runs\n",
    "           concatenated_ts = np.concatenate(subject_task_data, axis=1)\n",
    "\n",
    "           # Ensure frames are within bounds\n",
    "           max_timepoints = concatenated_ts.shape[1]\n",
    "           valid_frames = all_cond_frames[all_cond_frames < max_timepoints]\n",
    "           if len(valid_frames) == 0:\n",
    "               print(f\"No valid frames after clipping for {subject}/{task}/{condition}. Skipping.\")\n",
    "               continue\n",
    "\n",
    "           # Extract condition-specific timepoints\n",
    "           cond_ts = concatenated_ts[:, valid_frames]\n",
    "           task_data.append(cond_ts)\n",
    "           frame_counts.append(len(valid_frames))\n",
    "       except Exception as e:\n",
    "           print(f\"Error processing condition {condition} for {subject}/{task}: {e}\")\n",
    "           continue\n",
    "\n",
    "   # Stack subject arrays, truncating to minimum frame count\n",
    "   if not task_data:\n",
    "       print(f\"No data concatenated for task {task}, condition {condition}.\")\n",
    "       return None\n",
    "   try:\n",
    "       # Find minimum number of frames across subjects\n",
    "       min_frames = min(frame_counts) if frame_counts else 0\n",
    "       if min_frames == 0:\n",
    "           print(f\"No valid frames for task {task}, condition {condition}.\")\n",
    "           return None\n",
    "\n",
    "       # Truncate each subject's data to min_frames\n",
    "       truncated_data = [data[:, :min_frames] for data in task_data]\n",
    "\n",
    "       # Stack along a new axis (0) to get (n_subjects, n_parcels, n_condition_timepoints)\n",
    "       task_cond_array = np.stack(truncated_data, axis=0)\n",
    "       print(f\"Task {task}, Condition {condition}: Concatenated array shape {task_cond_array.shape}\")\n",
    "       return task_cond_array\n",
    "   except Exception as e:\n",
    "       print(f\"Error stacking subjects for task {task}, condition {condition}: {e}\")\n",
    "       return None\n",
    "\n",
    "# Concat\n",
    "task = \"WM\"\n",
    "conditions = EXPERIMENTS[task][\"cond\"]\n",
    "wm_data = {}\n",
    "SAVE_DIR = \"/home/amirreza/neuromatch/hcp_task/concatenated_data\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "for condition in conditions:\n",
    "    print(f\"Processing: {condition}\")\n",
    "    result = concatenate_task_data(task, condition, HCP_DIR)\n",
    "\n",
    "    if result is not None:\n",
    "        wm_data[condition] = result\n",
    "        print(f\"{condition} → shape: {result.shape}\")\n",
    "\n",
    "        # Save to disk right after creation\n",
    "        save_path = os.path.join(SAVE_DIR, f\"{condition}.npy\")\n",
    "        np.save(save_path, result)\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    else:\n",
    "        print(f\"No data for: {condition}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
